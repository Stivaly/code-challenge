# Proyecto ETL con BigQuery

Este proyecto consiste en un proceso ETL que extrae datos de un archivo CSV, los transforma y los carga en BigQuery. Fue desarrollado usando Python y las herramientas proporcionadas por Google Cloud Platform. Puedes ingresar al tablero de Jira donde se planificaron y abordaron los scripts en el siguiente link: https://id.atlassian.com/invite/p/jira-software?id=WwbBTysGTx2ZoQookUadeA selecciona *Mi proyecto Kanban* y podr谩s ingresar al tablero.

## Requisitos

- Python 3.11.3
- Google Cloud SDK
- [Otras dependencias se encuentran en `requirements.txt`]

# Referencia de los datos CSV

Online Retail. (2015). UCI Machine Learning Repository. https://doi.org/10.24432/C5BW33.

# Pasos tecnicos

En la carpeta docs del directorio encontrar谩s la arquitectura de software y el diagrama de flujo que se creo al inicio del proyecto. 

## Configuraci贸n del Ambiente

1. **Credenciales de GCP**: Coloque el archivo de credenciales `.json` proporcionado por GCP en el directorio `keys/`.
2. **Instalaci贸n de Google Cloud SDK**: Si a煤n no tiene instalado Google Cloud SDK en su m谩quina, siga las instrucciones en Google Cloud SDK Documentation para instalarlo.
3. **Autenticaci贸n**:
Una vez que tenga Google Cloud SDK instalado, ejecute el siguiente comando para autenticarse con su cuenta de Google:
    gcloud auth login

Luego, configure el proyecto de GCP con el siguiente c贸digo:
    gcloud config set project [etl-code-challenge]

4. **Configuraci贸n de las Credenciales**:

Establezca la variable de entorno GOOGLE_APPLICATION_CREDENTIALS en su m谩quina apuntando a la ubicaci贸n del archivo de credenciales.

En Windows (Powershell):
    $env:GOOGLE_APPLICATION_CREDENTIALS="C:\ruta\completa\al\archivo\de\credenciales.json"

En Linux o Mac:
    export GOOGLE_APPLICATION_CREDENTIALS="/ruta/completa/al/archivo/de/credenciales.json"

 Nota: Es importante establecer esta variable de entorno. Si est谩s trabajando con un entorno de desarrollo espec铆fico es posible que solo necesites hacerlo una vez por sesi贸n.

5. **Instalar Dependencias**: Ejecute el siguiente comando para instalar todas las dependencias necesarias: 
    pip install -r requirements.txt

## C贸mo ejecutar las consultas SQL

Ingresa al siguiente link para ejecutar las consultas de agregaci贸n y segmentaci贸n del proyecto directamente en BigQuery, luego de ejecutar el proceso ETL: 
    https://console.cloud.google.com/bigquery?sq=877558642730:c95480eb2fbd431d83e9d343fc874147


# Automatizaci贸n con Task Scheduler en Windows

Para automatizar, debido a que enfrent茅 muchos inconvenientes configurando tanto airflow como nncron, utilizo el Programador de tareas (Task Scheduler) en Windows. 

## Pasos para programar la automatizaci贸n:

1. Abre el Programador de tareas de Windows. Puedes encontrarlo en el men煤 "Herramientas administrativas" o buscarlo en el men煤 Inicio.

2. En el panel izquierdo, selecciona "Biblioteca del Programador de tareas" para crear una nueva tarea.

3. En la ventana derecha, haz clic en "Crear tarea b谩sica" para comenzar la configuraci贸n.

4. Dale un nombre a la tarea y proporciona una descripci贸n opcional. Luego, haz clic en "Siguiente".

5. Selecciona la opci贸n "Diariamente" o elige una frecuencia que se ajuste a tus necesidades y haz clic en "Siguiente".

6. Establece la hora y la fecha en la que deseas que se ejecute la tarea diariamente. Haz clic en "Siguiente".

7. En la siguiente pantalla, selecciona "Iniciar un programa" y haz clic en "Siguiente".

8. En el campo "Programa o script", proporciona la ruta completa al ejecutable de Python del entorno virtual. Ruta relativa desde el repositorio: 
    codeChallenge\venv\Scripts\python.exe

9. En el campo "Agregar argumentos (opcional)", debes proporcionar la ruta del script. Ruta relativa desde el repositorio:
    codeChallenge\src\etl_script.py

10. Haz clic en "Finalizar" para crear la tarea.

11. Ver谩s la tarea en la lista de tareas programadas. Puedes hacer clic derecho en ella y seleccionar "Ejecutar" para probarla de inmediato.

12. La tarea se ejecutar谩 autom谩ticamente seg煤n la programaci贸n que hayas establecido.


## Informaci贸n Adicional

*Notas sobre la Carga de Datos en BigQuery*

Durante el proceso, me enfrent茅 a un desaf铆o inesperado al cargar un archivo de datos en BigQuery. A pesar de que BigQuery es capaz de manejar grandes conjuntos de datos, encontr茅 problemas espec铆ficos con el archivo en cuesti贸n. Despu茅s de realizar diversas pruebas y diagn贸sticos, determin茅 que el problema podr铆a estar relacionado con la calidad, estructura o ciertos registros dentro del archivo. 

Para abordar esta situaci贸n, decid铆 reducir el tama帽o del archivo, excluyendo registros problem谩ticos. Esto me permiti贸 cargar una porci贸n m谩s manejable y limpia del conjunto de datos en BigQuery sin dificultades. Esta estrategia es 煤til cuando no es pr谩ctico analizar detalladamente cada registro en un conjunto de datos grande y se necesita cargar una parte representativa o dividir el archivo en partes iguales para una carga m谩s efectiva.

*Sobre Ramificaci贸n y Fusi贸n en Git*

Durante el desarrollo del proyecto, opt茅 por no utilizar la funcionalidad de ramificaci贸n y fusi贸n de Git debido a la naturaleza peque帽a y directa del proyecto. En lugar de crear ramas adicionales, centr茅 mi enfoque en mantener un flujo de trabajo eficiente mediante la r谩pida identificaci贸n y correcci贸n de errores. 

Aunque reconozco la importancia de las ramas en proyectos m谩s grandes, para este proyecto consider茅 que pod铆a ofrecer una soluci贸n de calidad sin necesidad de utilizarlas. Estoy abierta a la retroalimentaci贸n y siempre dispuesta a aprender y mejorar en futuros proyectos.

## Contribuciones

Este proyecto fue desarrollado como parte de un desaf铆o t茅cnico. Cualquier feedback es bienvenido.

## Contacto

Stivaly Gomez - stivaly.g@hotmail.com